"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([[1456],{6889:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"artificial-intelligence/natural-language-processing/byte-pair-encoding","title":"Byte Pair Encoding (BPE)","description":"A comprehensive guide to understanding Byte Pair Encoding (BPE), a subword tokenization algorithm widely used in modern NLP models.","source":"@site/docs/artificial-intelligence/natural-language-processing/byte-pair-encoding.md","sourceDirName":"artificial-intelligence/natural-language-processing","slug":"/artificial-intelligence/natural-language-processing/byte-pair-encoding","permalink":"/docs/artificial-intelligence/natural-language-processing/byte-pair-encoding","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"BPE","permalink":"/docs/tags/bpe"},{"inline":true,"label":"Byte Pair Encoding","permalink":"/docs/tags/byte-pair-encoding"},{"inline":true,"label":"ChatGPT","permalink":"/docs/tags/chat-gpt"}],"version":"current","lastUpdatedBy":"ShigureDD","lastUpdatedAt":1756656000000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1,"description":"A comprehensive guide to understanding Byte Pair Encoding (BPE), a subword tokenization algorithm widely used in modern NLP models.","author":"ShigureDD","tags":["BPE","Byte Pair Encoding","ChatGPT"],"keywords":["BPE","o200k_base","tiktoken"],"last_update":{"date":"2025/09/01 GMT+8","author":"ShigureDD"}},"sidebar":"docs","previous":{"title":"Natural Language Processing","permalink":"/docs/category/natural-language-processing"},"next":{"title":"Programming Langauge","permalink":"/docs/category/programming-langauge"}}');var o=i(4848),s=i(8453);const a={sidebar_position:1,description:"A comprehensive guide to understanding Byte Pair Encoding (BPE), a subword tokenization algorithm widely used in modern NLP models.",author:"ShigureDD",tags:["BPE","Byte Pair Encoding","ChatGPT"],keywords:["BPE","o200k_base","tiktoken"],last_update:{date:"2025/09/01 GMT+8",author:"ShigureDD"}},r="Byte Pair Encoding (BPE)",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"How BPE Works",id:"how-bpe-works",level:2},{value:"Example",id:"example",level:3},{value:"Implementation in NLP",id:"implementation-in-nlp",level:2},{value:"Common Implementations",id:"common-implementations",level:3},{value:"Applications",id:"applications",level:2},{value:"Advantages and Limitations",id:"advantages-and-limitations",level:2},{value:"Advantages",id:"advantages",level:3},{value:"Limitations",id:"limitations",level:3},{value:"Encoding",id:"encoding",level:2},{value:"ZH-CN Tokens extension",id:"zh-cn-tokens-extension",level:3},{value:"Code Example",id:"code-example",level:2},{value:"JavaScript (Node)",id:"javascript-node",level:3},{value:"Python",id:"python",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"byte-pair-encoding-bpe",children:"Byte Pair Encoding (BPE)"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Byte Pair Encoding (BPE) is a subword tokenization algorithm that has become a fundamental component in modern Natural Language Processing (NLP). Originally developed for data compression, BPE was later adapted for use in machine translation and has since become a standard preprocessing step for many state-of-the-art language models."}),"\n",(0,o.jsx)(n.h2,{id:"how-bpe-works",children:"How BPE Works"}),"\n",(0,o.jsx)(n.p,{children:"The BPE algorithm works by iteratively merging the most frequent pair of adjacent symbols (bytes or characters) in a text corpus. Here's a step-by-step breakdown:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Initialize Vocabulary"}),": Start with a vocabulary of individual characters and their frequencies."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Count Pairs"}),": Count the frequency of all adjacent symbol pairs in the vocabulary."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Merge Most Frequent Pair"}),": Replace the most frequent pair with a new symbol."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Update Vocabulary"}),": Add the new symbol to the vocabulary."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Repeat"}),": Continue the process until a desired vocabulary size is reached or no more merges are possible."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,o.jsx)(n.p,{children:"Consider the following text with word frequencies:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'"low" (5), "lower" (2), "newest" (6), "widest" (3)\n'})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Initial vocabulary: ",(0,o.jsx)(n.code,{children:"{l, o, w, e, r, n, s, i, d, t}"})]}),"\n",(0,o.jsxs)(n.li,{children:["After first merge (most frequent pair 'e' and 's'): ",(0,o.jsx)(n.code,{children:"{l, o, w, e, r, n, es, i, d, t}"})]}),"\n",(0,o.jsx)(n.li,{children:"Continue merging until desired vocabulary size is reached."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"implementation-in-nlp",children:"Implementation in NLP"}),"\n",(0,o.jsx)(n.p,{children:"BPE is particularly useful for NLP because it:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Handles out-of-vocabulary words by breaking them into known subword units"}),"\n",(0,o.jsx)(n.li,{children:"Reduces vocabulary size while maintaining meaningful representations"}),"\n",(0,o.jsx)(n.li,{children:"Balances between word-level and character-level representations"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"common-implementations",children:"Common Implementations"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"SentencePiece"}),": Google's implementation that supports both BPE and unigram language model tokenization."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hugging Face Tokenizers"}),": Provides BPE implementation along with other tokenization algorithms."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Subword NMT"}),": The original implementation for neural machine translation."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,o.jsx)(n.p,{children:"BPE is widely used in:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Pre-training language models (e.g., GPT, BERT)"}),"\n",(0,o.jsx)(n.li,{children:"Neural machine translation"}),"\n",(0,o.jsx)(n.li,{children:"Text generation tasks"}),"\n",(0,o.jsx)(n.li,{children:"Any NLP task requiring subword tokenization"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"advantages-and-limitations",children:"Advantages and Limitations"}),"\n",(0,o.jsx)(n.h3,{id:"advantages",children:"Advantages"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Handles rare words effectively"}),"\n",(0,o.jsx)(n.li,{children:"Reduces vocabulary size"}),"\n",(0,o.jsx)(n.li,{children:"Maintains meaningful subword units"}),"\n",(0,o.jsx)(n.li,{children:"Language-agnostic"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"limitations",children:"Limitations"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"May split words in unintuitive ways"}),"\n",(0,o.jsx)(n.li,{children:"Requires preprocessing and training on a corpus"}),"\n",(0,o.jsx)(n.li,{children:"Fixed vocabulary size must be determined in advance"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"encoding",children:"Encoding"}),"\n",(0,o.jsxs)(n.p,{children:["Use the following prebuilt encodings. For modern models like GPT-4o and GPT-4.1, prefer ",(0,o.jsx)(n.code,{children:"o200k_base"}),". For GPT-4 and GPT-3.5, use ",(0,o.jsx)(n.code,{children:"cl100k_base"}),"."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://unpkg.com/gpt-tokenizer/dist/o200k_base.js",children:"o200k_base (modern models: GPT-4o, GPT-4.1, o1, etc.)"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://unpkg.com/gpt-tokenizer/dist/cl100k_base.js",children:"cl100k_base (GPT-4, GPT-3.5)"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://unpkg.com/gpt-tokenizer/dist/p50k_base.js",children:"p50k_base"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://unpkg.com/gpt-tokenizer/dist/p50k_edit.js",children:"p50k_edit"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://unpkg.com/gpt-tokenizer/dist/r50k_base.js",children:"r50k_base"})}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"zh-cn-tokens-extension",children:"ZH-CN Tokens extension"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://gist.github.com/ctlllll/4451e94f3b2ca415515f3ee369c8c374",children:"Chinese tokenization extension (community)"})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"code-example",children:"Code Example"}),"\n",(0,o.jsx)(n.h3,{id:"javascript-node",children:"JavaScript (Node)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"npm install gpt-tokenizer\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:'import { encode, decode } from "gpt-tokenizer/encoding/o200k_base";\n\nconst text = "Hello, \u4e16\u754c!";\nconst tokens = encode(text);\n\nconsole.log("token count:", tokens.length);\nconsole.log("tokens:", tokens);\nconsole.log("decoded:", decode(tokens));\n\n// If you need GPT-4/3.5 compatibility instead:\n// import { encode as encodeCl100k } from \'gpt-tokenizer/encoding/cl100k_base\';\n// const tokensCl = encodeCl100k(text);\n'})}),"\n",(0,o.jsx)(n.h3,{id:"python",children:"Python"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install tiktoken\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import tiktoken\n\nenc = tiktoken.get_encoding("o200k_base")\ntext = "Hello, \u4e16\u754c!"\ntokens = enc.encode(text)\n\nprint("token count:", len(tokens))\nprint("tokens:", tokens)\nprint("decoded:", enc.decode(tokens))\n\n# For GPT-4/3.5 models:\n# enc = tiktoken.get_encoding("cl100k_base")\n'})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"Tip: Token counts are model-dependent. Always match the encoding to your target model."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"BPE provides a practical compromise between character- and word-level tokenization, enabling robust handling of rare and novel words while keeping the vocabulary manageable for efficient training and inference."}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://github.com/openai/tiktoken/tree/main",children:"OpenAI TikToken"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://github.com/dqbd/tiktokenizer",children:"Tiktokenizer"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://huggingface.co/docs/tokenizers/",children:"Hugging Face Tokenizers Documentation"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://huggingface.co/learn/llm-course/chapter6/5?fw=pt",children:"Hugging Face Byte-Pair Encoding tokenization"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://github.com/niieani/gpt-tokenizer",children:"GPT Tokenizer"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://blog.bytebytego.com/p/how-llms-see-the-world",children:"How LLMs See the World"})}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Last updated: August 2025"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);